{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XL(3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Dear Flan Lovers, I just wanted to let you know that our alpaca named Oscar likes his flan! He is so much fun and loves the food sooo much he decided to make them for us! He really thinks he's a star chef! Please stop by soon to get some flan and meet our latest feathered friend! Welcome to Flan Lovers!' Analpaca Manager'\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"Write an email about an alpaca that likes flan\"\n",
    "model = pipeline(model=\"declare-lab/flan-alpaca-xxl\")\n",
    "model(prompt, max_length=128, do_sample=True)\n",
    "\n",
    "# Dear AlpacaFriend,\n",
    "# My name is Alpaca and I'm 10 years old.\n",
    "# I'm excited to announce that I'm a big fan of flan!\n",
    "# We like to eat it as a snack and I believe that it can help with our overall growth.\n",
    "# I'd love to hear your feedback on this idea. \n",
    "# Have a great day! \n",
    "# Best, AL Paca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Machine learning and deep learning are both powerful tools used in the field of computer science. However, there are key differences between these two approaches. Machine learning is a pedagogical approach to teaching computers and algorithms how to learn and improve from experience. It focuses on building models that can analyze and interpret data in order to make predictions or take actions based on that data. Machine learning often involves the use of algorithms to make models that can solve problems, predict outcomes, and classify data. Deep learning, on the other hand, is an approach to teaching computers to learn and understand from data using neural networks, which are'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is difference between machine learning and deep learning?\"\n",
    "model(prompt, max_length=128, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Based on the current evidence, the current situation and the circumstances of the fight between the lion and the tiger are conflicting. While the lion was initially larger and stronger than the tiger, over time, with proper training and protection measures, it may have been able to defeat the tiger by being more agile and superior in fighting technique.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"If lion and tiger fight, who wins?\"\n",
    "model(prompt, max_length=128, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXL(11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = \"What is difference between machine learning and deep learning?\"\n",
    "model = pipeline(model=\"declare-lab/flan-alpaca-xxl\")\n",
    "model(prompt, max_length=128, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Lions are bigger and stronger, and have a greater range of motion. Tigers are smaller and faster, making them easier to handle.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"If lion and tiger fight, who wins?\"\n",
    "model(prompt, max_length=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I would propose a new idea for natural language processing paper that uses neural networks for semantic analysis of text and natural language to detect and predict the sentiment and sentiment context of natural language. The paper would leverage deep learning architectures to capture context and sentiment of natural language in form of data, and then present personalized recommendations and responses that are tailored to each user's unique needs.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Tell me new idea about natural laguage processing paper.\"\n",
    "model(prompt, max_length=256, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/miniconda3/envs/yys/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)l-00003-of-00005.bin: 100%|██████████| 9.96G/9.96G [07:46<00:00, 21.3MB/s]\n",
      "Downloading (…)l-00004-of-00005.bin: 100%|██████████| 10.0G/10.0G [08:10<00:00, 20.4MB/s]\n",
      "Downloading (…)l-00005-of-00005.bin: 100%|██████████| 6.06G/6.06G [04:46<00:00, 21.2MB/s]\n",
      "Downloading shards: 100%|██████████| 5/5 [20:47<00:00, 249.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.82s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 142/142 [00:00<00:00, 30.8kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.50k/2.50k [00:00<00:00, 1.52MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 2.66MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 1.32MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-xxl\",\n",
    "                                             device_map=\"auto\",\n",
    "                                             revision=\"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-xxl\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generate_text =pipeline(model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        task='text-generation',\n",
    "                        trust_remote_code=True,\n",
    "                        device_map=\"auto\",\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.95,\n",
    "                        repetition_penalty=1.15,\n",
    "                        max_new_tokens=256,\n",
    "                        return_full_text = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWrite an email about an alpaca that likes flan\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m pipeline(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeclare-lab/flan-alpaca-xxl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model(prompt, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email about an alpaca that likes flan\"\n",
    "model = pipeline(model=\"declare-lab/flan-alpaca-xxl\")\n",
    "model(prompt, max_length=128, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b414fc1e9b01ec0e3104513ada18687df2a6952cdc4efdeda01cb7a7dbe427c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
