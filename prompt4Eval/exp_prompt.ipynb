{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.cuda\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from model import LMPrompt4Eval\n",
    "from prepro_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    answer = ['terrible','poor','average','good','excellent']\n",
    "    answer_str = ' '.join(answer)  # 리스트를 공백으로 구분된 문자열로 변환\n",
    "    answer_ids = tokenizer.encode(answer_str, add_special_tokens=False)\n",
    "\n",
    "    net = LMPrompt4Eval(model_name, answer_ids, args)\n",
    "    return net, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/work/deeptext/yys/redpen/prompt4Eval/prompt_models/deberta-v3-base/readability/model_readability.pt')\n",
    "model.to(device)\n",
    "test_dataset = MyDataset(args,tokenizer=tokenizer,path=f'/home/work/deeptext/yys/redpen/data/test/{args.ability}_prompt_test.json' )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12 (main, Jul  5 2023, 18:54:27) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8e99decc2081de849a29470fa13db5bdc133f36de876237d16680bb689c064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
